{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNkNxPZ6Gt02Q0HQ5cpgbLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nova0816/RAG_pipeline/blob/main/RAG_Pipeline_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b32c24c1"
      },
      "source": [
        "## RAG Pipeline for Car Rental Information\n",
        "\n",
        "### 1. Objective\n",
        "This notebook demonstrates a Retrieval Augmented Generation (RAG) pipeline designed to answer questions based on information extracted from a Sixt Rental Information (France) HTML document. It uses LangChain, HuggingFace models (for embeddings and generation), and FAISS for vector storage.\n",
        "\n",
        "### 2. How to Use\n",
        "1.  **Install Libraries**: Run the first code cell to install all necessary Python packages.\n",
        "2.  **Mount Google Drive & Specify File Path**: Ensure your Google Drive is mounted and update the `file_path` variable in the 'FILE INGESTION' section to point to your `Sixt Rental Information France.html` file.\n",
        "3.  **Run All Cells**: Execute all code cells sequentially from top to bottom.\n",
        "4.  **Query the Model**: Modify the `query` variable in the 'Final Question and Answer' section and run the cell to get answers from the RAG pipeline.\n",
        "\n",
        "### 3. Code Structure\n",
        "The notebook is organized into the following key sections:\n",
        "*   **Library Installation**: Installs `langchain`, `pypdf`, `sentence-transformers`, `faiss-cpu`, `transformers`, etc.\n",
        "*   **File Ingestion**: Mounts Google Drive and reads the specified HTML document.\n",
        "*   **HTML Parsing and Text Linearization**: Converts the HTML content into a structured, readable text format, extracting headings, paragraphs, and converting tables to Markdown.\n",
        "*   **Chunking and LangChain Document Creation**: Splits the cleaned text into smaller chunks and converts them into LangChain `Document` objects.\n",
        "*   **Embedding and Vector Store**: Uses `HuggingFaceEmbeddings` (e.g., `BAAI/bge-small-en-v1.5`) to create embeddings from the chunks and stores them in a `FAISS` vector store for efficient retrieval.\n",
        "*   **RAG Pipeline Definition**: Sets up the RAG chain using LangChain Expression Language (LCEL) with a prompt template and a HuggingFace LLM (initially `google/flan-t5-small`, then `meta-llama/Meta-Llama-3-8B-Instruct`).\n",
        "*   **Model Testing**: Demonstrates how to query the RAG pipeline and retrieve answers using both a smaller `google/flan-t5-small` model and a more powerful `meta-llama/Meta-Llama-3-8B-Instruct` model (with 4-bit quantization)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LLM and chain modules\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_core.prompts import ChatPromptTemplate # Using ChatPromptTemplate for modern RAG\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "import torch # Required for device management\n",
        "\n",
        "# New imports for LCEL\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- GENERATION ---\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Load a small LLM (FLAN-T5 is a good choice for text2text generation)\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    device=device # Use GPU if available\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create the RAG chain using LCEL\n",
        "# Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context.\n",
        "{context}\n",
        "If there's ambiguity there, encourge the user to be more specific about the questions.\n",
        "Question: {question}\"\"\")\n",
        "\n",
        "# Construct the RAG chain using LCEL\n",
        "qa_chain = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- Final Question and Answer ---\n",
        "#final_query = \"How is the cancellation policy?\"\n",
        "result = qa_chain.invoke(query) # LCEL invoke directly with question string\n",
        "\n",
        "# Post-process the result to remove extraneous Document information\n",
        "if \"), Document(id='\" in result:\n",
        "    result = result.split(\"'), Document(\")[0].strip() + \"'\"\n",
        "\n",
        "print(f\"\\n--- User Query ---\\n{query}\")\n",
        "print(f\"\\n--- Generated Answer ---\\n{result}\")"
      ],
      "metadata": {
        "id": "pzCU8I0hvW4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Libraries (Run this cell in Colab)\n",
        "!pip install --upgrade langchain pypdf sentence-transformers faiss-cpu transformers langchain-text-splitters langchain-huggingface langchain-community\n",
        "# Install the necessary libraries\n",
        "!pip install -q transformers accelerate bitsandbytes sentence-transformers langchain torch\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "0wwezuTUVCe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries (Run this in your Colab notebook)\n",
        "!pip install beautifulsoup4 pandas html5lib langchain"
      ],
      "metadata": {
        "id": "qJywRZZIZX3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXuZnhjnoEMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read file"
      ],
      "metadata": {
        "id": "Xd8nRpZPoEhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to the default mount point /content/drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully.\")"
      ],
      "metadata": {
        "id": "S8IQFwbooIRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries (Run this cell in Colab)\n",
        "#!pip install beautifulsoup4 pandas html5lib langchain\n",
        "\n",
        "# Import necessary modules\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from io import StringIO\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "\n",
        "# --- FILE INGESTION (Direct Drive Read) ---\n",
        "\n",
        "# **IMPORTANT**: Define the full path to your file based on the screenshot structure\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/RAG Material/Sixt Rental Information France.html\"\n",
        "html_content = \"\"\n",
        "\n",
        "# Check for the file and read its content\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        # Read the file content.\n",
        "        # Using 'windows-1252' encoding as suggested for this file type\n",
        "        with open(file_path, 'r', encoding='windows-1252') as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        print(f\"File '{file_path}' successfully loaded.\")\n",
        "        print(f\"Size of loaded HTML content: {len(html_content)} characters.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\")\n",
        "        html_content = \"<html><body>Error reading content.</body></html>\"\n",
        "else:\n",
        "    print(f\"Error: File not found at the expected path: {file_path}\")\n",
        "    print(\"Please ensure the Drive is mounted and the path is exactly correct.\")\n",
        "    html_content = \"<html><body>File is missing.</body></html>\"\n",
        "\n",
        "\n",
        "print(\"\\n--- Setup Complete ---\")"
      ],
      "metadata": {
        "id": "X3Fc5dCMnZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_content"
      ],
      "metadata": {
        "id": "-cgNK_MApLUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Html Document"
      ],
      "metadata": {
        "id": "kXwt3HSAZYP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "from io import StringIO\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "## 2. Table Extraction and Text Linearization (FIXED)\n",
        "\n",
        "def parse_html_for_rag(html_content):\n",
        "    \"\"\"\n",
        "    Parses HTML by finding all structural elements (headings, paragraphs, tables)\n",
        "    within the main content div, converts tables to Markdown, and cleans text.\n",
        "\n",
        "    The fix targets the inner <div> blocks that contain the actual content structure.\n",
        "    \"\"\"\n",
        "    if not html_content:\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Target the main content area\n",
        "    content_div = soup.find('div', id='sx-gc-content')\n",
        "    if not content_div:\n",
        "        return \"Error: Could not find main content div (sx-gc-content).\"\n",
        "\n",
        "    processed_text_segments = []\n",
        "\n",
        "    # Process the header text right after the main div start, but before the inner divs\n",
        "    header_text = content_div.find('h3', class_='sx-terms-header-selection')\n",
        "    if header_text:\n",
        "        processed_text_segments.append(f\"\\n# {header_text.get_text(strip=True)}\\n\")\n",
        "\n",
        "\n",
        "    # The actual content is nested inside subsequent sibling <div> tags\n",
        "    # We look for all immediate child <div> tags that contain the sections\n",
        "    content_sections = content_div.find_all('div', recursive=False)\n",
        "\n",
        "    # Iterate through each section div found\n",
        "    for section in content_sections:\n",
        "\n",
        "        # Iterate through all children of the inner section to maintain order\n",
        "        for element in section.children:\n",
        "            if isinstance(element, Tag):\n",
        "\n",
        "                # Handle Tables: Extract and convert to Markdown\n",
        "                if element.name == 'table':\n",
        "                    try:\n",
        "                        # Use pandas to extract the table data\n",
        "                        dfs = pd.read_html(StringIO(str(element)), flavor='bs4')\n",
        "                        if dfs:\n",
        "                            df = dfs[0]\n",
        "                            # Convert the DataFrame to a Markdown string\n",
        "                            markdown_table = df.to_markdown(index=False)\n",
        "\n",
        "                            processed_text_segments.append(\n",
        "                                f\"\\n\\n--- Start Table ---\\n{markdown_table}\\n--- End Table ---\\n\\n\"\n",
        "                            )\n",
        "                    except ValueError:\n",
        "                        processed_text_segments.append(f\"\\n\\n[Could not parse complex table]\\n\\n\")\n",
        "\n",
        "                # Handle Headings and Paragraphs: Extract text\n",
        "                elif element.name in ['h2', 'h3', 'p']:\n",
        "                    text = element.get_text(strip=True)\n",
        "                    # Add separators for better readability and chunking context\n",
        "                    if element.name == 'h2':\n",
        "                        text = f\"\\n\\n## {text}\"\n",
        "                    elif element.name == 'h3':\n",
        "                        text = f\"\\n### {text}\"\n",
        "\n",
        "                    # Also include any links' text for context, but not the link itself\n",
        "                    if element.name == 'p':\n",
        "                        # Find all <a> tags and extract their text\n",
        "                        for a in element.find_all('a'):\n",
        "                             a_text = a.get_text(strip=True)\n",
        "                             if a_text not in text: # Avoid double counting text already captured by parent\n",
        "                                 text += f\" ({a_text})\"\n",
        "\n",
        "                    processed_text_segments.append(text)\n",
        "\n",
        "    # Combine all segments into one large, structured string\n",
        "    full_cleaned_text = \"\\n\".join(processed_text_segments).strip()\n",
        "\n",
        "    return full_cleaned_text\n",
        "\n",
        "# Execute the parsing function with the loaded content\n",
        "cleaned_rag_text = parse_html_for_rag(html_content)\n",
        "\n",
        "print(f\"\\n--- Parsing Results ---\")\n",
        "print(f\"Total characters in cleaned text: {len(cleaned_rag_text)}\")\n",
        "print(\"\\n--- Example of Cleaned Text (First 1500 chars) ---\")\n",
        "print(cleaned_rag_text[:1500])\n",
        "print(\"\\n--- Parsing and Linearization Complete ---\")\n",
        "\n",
        "\n",
        "## 3. Chunking and LangChain Document Creation\n",
        "\n",
        "# Convert the cleaned text into a single LangChain Document\n",
        "document = Document(page_content=cleaned_rag_text, metadata={\"source\": \"Sixt Rental Info France\"})\n",
        "\n",
        "# Define the text splitter for RAG chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "# Split the document into chunks for embedding\n",
        "final_chunks = text_splitter.split_documents([document])\n",
        "\n",
        "print(f\"\\nTotal text chunks created: {len(final_chunks)}\")\n",
        "print(\"\\n--- Example Chunk Content with Table Data ---\")\n",
        "# Print the first chunk to show structure\n",
        "print(final_chunks[0].page_content)\n",
        "print(\"\\n--- Another Chunk (Searching for a table) ---\")\n",
        "# Find a chunk that contains a table for demonstration\n",
        "for chunk in final_chunks:\n",
        "    if 'Start Table' in chunk.page_content:\n",
        "        print(chunk.page_content)\n",
        "        break"
      ],
      "metadata": {
        "id": "NMokUCPyZbnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the cleaned text into a single LangChain Document\n",
        "document = Document(page_content=cleaned_rag_text, metadata={\"source\": \"Sixt Rental Info France\"})\n",
        "\n",
        "# Define the text splitter for RAG chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "# Split the document into chunks for embedding\n",
        "final_chunks = text_splitter.split_documents([document])\n",
        "\n",
        "print(f\"\\nTotal text chunks created: {len(final_chunks)}\")\n",
        "print(\"\\n--- Example Chunk Content with Table Data ---\")\n",
        "# Find a chunk that contains a table for demonstration\n",
        "for chunk in final_chunks:\n",
        "    if 'Start Table' in chunk.page_content:\n",
        "        print(chunk.page_content)\n",
        "        break\n",
        "# If no table found in the first few chunks, print the first chunk\n",
        "else:\n",
        "    print(final_chunks[0].page_content)"
      ],
      "metadata": {
        "id": "zU4BcKYBZbku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG Pipeline"
      ],
      "metadata": {
        "id": "JnXjgDOpZpZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document # Used to convert text to LangChain Document format\n",
        "\n",
        "# Import embedding and vector store modules\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "awaATztjrcAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EMBEDDING & RETRIEVAL (Vector Store) ---\n",
        "\n",
        "# Choose an open-source embedding model\n",
        "embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "#sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Create a FAISS vector store from the chunks\n",
        "vector_store = FAISS.from_documents(final_chunks, embeddings)\n",
        "\n",
        "print(\"Vector Store created and documents embedded.\")\n",
        "print(\"--- Embedding and Indexing Complete ---\")\n"
      ],
      "metadata": {
        "id": "-oT1lPTbvUZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10}) # Retrieve top 2 chunks\n",
        "\n",
        "# Example Retrieval\n",
        "query = \" I just booked and want to cancel, can I get a refund?\"\n",
        "retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "print(f\"\\n--- Retrieved Context (Top {len(retrieved_docs)} Chunks) ---\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"Chunk {i+1}: {doc.page_content}...\")"
      ],
      "metadata": {
        "id": "Lc14CdzYtaYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model 1: Generate answer by model google/flan-t5-small, a text model"
      ],
      "metadata": {
        "id": "pZvfvQF5bwUI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzYHWkLjtRur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Model 2: Generate model by Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "id": "21f6qYcQcJP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import necessary modules\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# --- Model Selection and Configuration ---\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# Alternatively, use a smaller but capable model:\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Check for GPU and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration for 4-bit quantization (REQUIRED to fit 8B/7B models on Colab T4/P100)\n",
        "# This reduces memory footprint by loading the model in 4-bit precision.\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "jJ68bct1cFR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01bfd198"
      },
      "source": [
        "# Install huggingface_hub if not already installed\n",
        "!pip install -q huggingface_hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loading model: {model_name}...\")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Set pad token for batching (best practice)\n",
        "\n",
        "# Load Model with Quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16, # Use bfloat16 for computation\n",
        ")\n",
        "\n",
        "# Create the text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,  # Set a higher limit for comprehensive answers\n",
        "    temperature=0.1,     # Low temperature for factual RAG answers\n",
        "    do_sample=True,      # Enable sampling for varied responses\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "# Wrap the pipeline in a LangChain LLM object\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"LLM and Pipeline Loaded Successfully.\")"
      ],
      "metadata": {
        "id": "qQ6GSIdVcYbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e52b5771"
      },
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the token from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=hf_token)\n",
        "\n",
        "print(\"Hugging Face authentication complete. You can now try loading the gated model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context.\n",
        "{context}\n",
        "If there's ambiguity there, encourge the user to be more specific about the questions.\n",
        "Also, please quote the title of the section where the answer is found.\n",
        "Question: {question}\"\"\")\n",
        "\n",
        "# Construct the RAG chain using LCEL\n",
        "qa_chain = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Example Query\n",
        "result = qa_chain.invoke(query) # LCEL invoke directly with question string\n",
        "\n",
        "# Post-process the result to remove extraneous Document information if any\n",
        "# This specific post-processing might be less relevant for text-generation LLMs but kept for consistency.\n",
        "if \"), Document(id='\" in result:\n",
        "    result = result.split(\"')', Document(\")[0].strip() + \"'\"\n",
        "\n",
        "print(f\"\\n--- User Query ---\\n{query}\")\n",
        "print(f\"\\n--- Generated Answer ---\\n{result}\")"
      ],
      "metadata": {
        "id": "uI2MnQp_ctnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hS8zMUOJdQR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}